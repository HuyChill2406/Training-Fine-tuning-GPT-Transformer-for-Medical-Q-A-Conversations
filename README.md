# Training-Fine-tuning-GPT-Transformer-for-Medical-Q-A-Conversations
This repository explores training and fine-tuning Transformer-based language models for medical question-answering dialogues, including GPT-style causal models and encoder-decoder Transformers. Experiments compare scratch training and pretrained fine-tuning using a large-scale healthcare chat dataset.
